{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import gc\n",
    "from yt_dlp import YoutubeDL\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "# Function to load the CSV file into a DataFrame\n",
    "def load_csv_as_dataframe(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CSV file: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to download YouTube videos and save to specified path\n",
    "def download_youtube_videos(df, video_id_column, save_path, batch_size=10):\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    \n",
    "    failed_videos = set()  # Use a set to store video IDs that can't be downloaded\n",
    "    video_details = {}\n",
    "\n",
    "    # Ensure the required columns exist\n",
    "    if 'video_title' not in df.columns:\n",
    "        df['video_title'] = None\n",
    "    if 'description' not in df.columns:\n",
    "        df['description'] = None\n",
    "    if 'transcriptions' not in df.columns:\n",
    "        df['transcriptions'] = None\n",
    "\n",
    "    ydl_opts = {\n",
    "        'outtmpl': f'{save_path}/%(id)s.%(ext)s',\n",
    "        'format': 'best',\n",
    "        'quiet': True\n",
    "    }\n",
    "\n",
    "    # Process in batches\n",
    "    for start in range(0, len(df), batch_size):\n",
    "        end = min(start + batch_size, len(df))\n",
    "        batch_df = df.iloc[start:end]\n",
    "        \n",
    "        for index, row in batch_df.iterrows():\n",
    "            video_id = row[video_id_column]\n",
    "            video_file_path = os.path.join(save_path, f\"{video_id}.mp4\")\n",
    "            \n",
    "            if video_id in failed_videos:\n",
    "                continue\n",
    "\n",
    "            if pd.isna(row['video_title']) or pd.isna(row['description']) or pd.isna(row['transcriptions']):\n",
    "                # Extract metadata and transcript\n",
    "                try:\n",
    "                    print(f\"Trying to get title and description for video ID {video_id} at index {index}\")\n",
    "                    with YoutubeDL({'quiet': True}) as ydl:\n",
    "                        info_dict = ydl.extract_info(f'https://www.youtube.com/watch?v={video_id}', download=False)\n",
    "                        title = info_dict.get('title', None)\n",
    "                        description = info_dict.get('description', None)\n",
    "\n",
    "                    # Attempt to get the transcript\n",
    "                    try:\n",
    "                        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "                        transcript_text = \" \".join([item['text'] for item in transcript])\n",
    "                    except Exception as e:\n",
    "                        transcript_text = f\"Transcript not available: {e}\"\n",
    "\n",
    "                    video_details[video_id] = {\n",
    "                        'title': title,\n",
    "                        'description': description,\n",
    "                        'transcript': transcript_text\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    print(f\"Error retrieving metadata for video {video_id}: {e}\")\n",
    "                    failed_videos.add(video_id)\n",
    "                    continue\n",
    "\n",
    "            # Download video if it doesn't exist\n",
    "            if not os.path.exists(video_file_path):\n",
    "                try:\n",
    "                    with YoutubeDL(ydl_opts) as ydl:\n",
    "                        ydl.download([f'https://www.youtube.com/watch?v={video_id}'])\n",
    "                    print(f\"Downloaded: {video_id}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error downloading video {video_id}: {e}\")\n",
    "                    failed_videos.add(video_id)\n",
    "                    continue\n",
    "        \n",
    "        # Add new columns to the DataFrame using the video_details dictionary\n",
    "        for video_id, details in video_details.items():\n",
    "            df.loc[df[video_id_column] == video_id, 'video_title'] = details['title']\n",
    "            df.loc[df[video_id_column] == video_id, 'description'] = details['description']\n",
    "            df.loc[df[video_id_column] == video_id, 'transcriptions'] = details['transcript']\n",
    "\n",
    "        # Clean up and free memory\n",
    "        del batch_df\n",
    "        gc.collect()\n",
    "    \n",
    "    return df, list(failed_videos)\n",
    "\n",
    "path = \"\"\n",
    "\n",
    "file_path = path + \"NEW_IMSyPP_EN_YouTube_comments_evaluation_context_1517_PREPROCESSED.csv\"\n",
    "video_id_column = 'video_id'\n",
    "save_path = path + \"NEW_Hatebase_dataset_downloaded_videos_THIRD_TEST\"\n",
    "intermediate_path = path + \"intermediate_csvs_TEST/\"\n",
    "\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "os.makedirs(intermediate_path, exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(file_path, sep=';')\n",
    "\n",
    "if df is not None:\n",
    "    updated_df, failed_videos = download_youtube_videos(df, video_id_column, save_path)\n",
    "    if failed_videos:\n",
    "        updated_df = updated_df[~updated_df[video_id_column].isin(failed_videos)]\n",
    "        updated_df.to_csv(path + 'NEW_IMSyPP_EN_YouTube_comments_evaluation_context_1517_PREPROCESSED_no_restricted_videos_inte_THIRD_TEST.csv', index=False)\n",
    "    updated_df.to_csv(path + 'NEW_IMSyPP_EN_YouTube_comments_evaluation_context_1517_PREPROCESSED_no_restricted_videos_THIRD_TEST.csv', index=False)  # Save final output\n",
    "    print(updated_df.head())\n",
    "    print(f\"Failed videos: {failed_videos}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#failed videos using  from yt_dlp import YoutubeDL are: 'lTSetpETzFA', '_MDo0UhgIys', '0gx2FJIITFo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df.to_csv(path + 'NEW_IMSyPP_EN_YouTube_comments_evaluation_context_1483_PREPROCESSED_no_restricted_videos_THIRD_TEST-redownloaded_with_yt_dlp.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = updated_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Set the options to display all rows and columns\n",
    "pd.set_option('display.max_rows', None)  # None means show all rows\n",
    "pd.set_option('display.max_columns', None)  # None means show all columns\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the DataFrame before removing rows\n",
    "print(\"DataFrame before removing empty 'video_title' rows:\")\n",
    "#print(df1)\n",
    "\n",
    "# Remove rows where 'video_title' is empty or None\n",
    "df2 = df1[df1['video_title'].notna() & (df1['video_title'] != '')]  \n",
    "df_cleaned =df2[df2['transcriptions'].notna() & (df2['transcriptions'] != '')]\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "print(\"\\nDataFrame after removing empty 'video_title' rows:\")\n",
    "print(len(df_cleaned))\n",
    "#df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove rows with \"Transcript not available:\" in the transcriptions column\n",
    "def remove_unavailable_transcripts(df, transcription_column='transcriptions'):\n",
    "    # Filter out rows where the transcription contains \"Transcript not available:\"\n",
    "    df_cleaned = df[~df[transcription_column].str.contains(\"Transcript not available:\", na=False)]\n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "\n",
    "df_cleaned_trans = remove_unavailable_transcripts(df_cleaned)\n",
    "len(df_cleaned_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to remove rows where \"type_reply\" is None or NaN\n",
    "def remove_none_nan_type_reply(df):\n",
    "    df_cleaned = df.dropna(subset=['type_reply'])\n",
    "    df_cleaned = df_cleaned.dropna(subset=['transcriptions'])\n",
    "    df_cleaned = df_cleaned.dropna(subset=['reply'])\n",
    "    df_cleaned = df_cleaned.dropna(subset=['comment'])\n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "df_trans_nans = remove_none_nan_type_reply(df_cleaned_trans)\n",
    "len(df_trans_nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "df_trans_nans_token=df_trans_nans.copy()\n",
    "\n",
    "# Tokenize each text and count the number of tokens\n",
    "df_trans_nans_token['token_count'] = df_trans_nans_token['transcriptions'].apply(lambda x: len(x.split()))\n",
    "df_trans_nans_token.head()\n",
    "\n",
    "# Print detailed statistics\n",
    "print(\"Detailed Statistics:\")\n",
    "print(df_trans_nans_token['token_count'].describe(percentiles=[.01, .05, .10, .25, .50, .75, .90, .95, .99]))\n",
    "\n",
    "# Calculate the number of bins for 2 token steps\n",
    "max_tokens = df_trans_nans_token['token_count'].max()\n",
    "bin_size = 2000\n",
    "num_bins = int((max_tokens / bin_size) + 1)\n",
    "\n",
    "# Create an interactive plot with more bars\n",
    "fig = px.histogram(df_trans_nans_token, x='token_count', nbins=num_bins, title='Granular Distribution of Token Counts per Transcription')\n",
    "\n",
    "# Update layout for better readability\n",
    "fig.update_layout(\n",
    "    xaxis_title='Number of Tokens',\n",
    "    yaxis_title='Frequency (Number of Texts)',\n",
    "    bargap=0.2,\n",
    ")\n",
    "\n",
    "# Add hover data\n",
    "fig.update_traces(\n",
    "    hovertemplate='Number of Tokens: %{x}<br>Frequency: %{y}<extra></extra>',\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize each text and count the number of tokens\n",
    "df_trans_nans_token['token_count'] = df_trans_nans_token['transcriptions'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Print the original DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "#print(df)\n",
    "\n",
    "# Remove rows where token count is greater than 2000\n",
    "df_trans_nans_token = df_trans_nans_token[df_trans_nans_token['token_count'] <= 2000]\n",
    "\n",
    "# Drop the token_count column as it's no longer needed\n",
    "df_trans_nans_token_FINAL = df_trans_nans_token.drop(columns=['token_count'])\n",
    "len(df_trans_nans_token_FINAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trans_nans_token_FINAL.to_csv('FINAL_IMSyPP_EN_697_PREPROCESSED_no_restricted_videos_features_2ktoken_THIRD_TEST-redownloaded_with_yt_dlp.csv', sep = ';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
